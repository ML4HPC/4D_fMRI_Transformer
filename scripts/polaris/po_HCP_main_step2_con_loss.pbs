#!/bin/sh
#PBS -l select=1:system=polaris
#PBS -l place=scatter
#PBS -l walltime=0:30:00
#PBS -l filesystems=home:grand
#PBS -q debug
#PBS -A STlearn

module load hpctoolkit/2022.07.27
module load conda
conda activate 3DCNN # activate conda 

cd $PBS_O_WORKDIR

export MASTER_ADDR=$(hostname)
'''
export RANK=$SLURM_PROCID
export LOCAL_RANK=$SLURM_LOCALID
export WORLD_SIZE=$SLURM_NTASKS
export MASTER_PORT=29500
'''

### get the first node name as master address - customized for vgg slurm
### e.g. master(gnodee[2-5],gnoded1) == gnodee2
#echo "NODELIST="${SLURM_NODELIST}
#master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
#export MASTER_ADDR=$master_addr
#echo "MASTER_ADDR="$MASTER_ADDR

NNODES=`wc -l < $PBS_NODEFILE`
NRANKS_PER_NODE=$(nvidia-smi -L | wc -l)
NDEPTH=8
NTHREADS=1


NTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))
echo "NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}"


#mpirun bash -c "
#source /lus/grand/projects/STlearn/4D_fMRI_Transformer/export_DDP_vars.sh  
#python /lus/grand/projects/STlearn/4D_fMRI_Transformer/main.py --step 2 --batch_size_phase2 2 --exp_name con_loss --use_cont_loss True --image_path /lus/grand/projects/STlearn/HCP_MNI_to_TRs --workers_phase2 4 "

mpirun -n $NNODES --hostfile $PBS_NODEFILE -np 2 ./run_HCP_main_step2_con_loss.sh