{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lus/grand/projects/STlearn/4D_fMRI_Transformer/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lus/grand/projects/STlearn/4D_fMRI_Transformer\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_model_summary\n",
    "import torch\n",
    "from modules.model import Encoder_Transformer_Decoder,Encoder_Transformer_finetune,AutoEncoder, MobileNet_v2_Transformer_finetune,MobileNet_v3_Transformer_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from modules.data_preprocess_and_load.data_module3 import fMRIDataModule\n",
    "import os\n",
    "def get_arguments(base_path):\n",
    "    \"\"\"\n",
    "    handle arguments from commandline.\n",
    "    some other hyper parameters can only be changed manually (such as model architecture,dropout,etc)\n",
    "    notice some arguments are global and take effect for the entire three phase training process, while others are determined per phase\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser(add_help=False, formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--exp_name', type=str,default=\"baseline\") \n",
    "    parser.add_argument('--base_path', default=base_path)\n",
    "    parser.add_argument('--step', default='1', choices=['1','2','3','4'], help='which step you want to run')\n",
    "    \n",
    "    parser.add_argument('--cuda', default=True)\n",
    "    parser.add_argument('--log_dir', type=str, default=os.path.join(base_path, 'runs'))\n",
    "    \n",
    "    \n",
    "    # parser.add_argument('--random_TR', action='store_false') #True면(인자를 넣어주지 않으면) 전체 sequence 로부터 random sampling(default). False면 (--random_TR 인자를 넣어주면) 0번째 TR부터 sliding window\n",
    "    \n",
    "    # loss-related\n",
    "    parser.add_argument('--intensity_factor', default=1)\n",
    "    parser.add_argument('--perceptual_factor', default=1)\n",
    "    parser.add_argument('--which_perceptual', default='vgg', choices=['vgg','densenet3d'])\n",
    "    parser.add_argument('--reconstruction_factor', default=1)\n",
    "    \n",
    "    # model related\n",
    "    parser.add_argument('--transformer_hidden_layers', type=int,default=16)\n",
    "    parser.add_argument('--transformer_num_attention_heads',type=int, default=16)\n",
    "    parser.add_argument('--transformer_emb_size',type=int ,default=2640)\n",
    "    parser.add_argument('--running_mean_size', default=5000)\n",
    "    \n",
    "    # DDP configs:\n",
    "    parser.add_argument('--world_size', default=-1, type=int, \n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int, \n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int, \n",
    "                        help='local rank for distributed training')\n",
    "    parser.add_argument('--dist_backend', default='nccl', type=str, \n",
    "                        help='distributed backend')\n",
    "    parser.add_argument('--init_method', default='env', type=str, choices=['file','env'], help='DDP init method')\n",
    "    \n",
    "\n",
    "    # AMP configs:\n",
    "    parser.add_argument('--amp', action='store_false')\n",
    "    parser.add_argument('--gradient_clipping', action='store_true')\n",
    "    #parser.add_argument('--opt_level', default='O1', type=str,\n",
    "    #                    help='opt level of amp. O1 is recommended')\n",
    "    \n",
    "    # Gradient accumulation\n",
    "    parser.add_argument(\"--accumulation_steps\", default=1, type=int,required=False,help='mini batch size == accumulation_steps * args.train_batch_size')\n",
    "    \n",
    "    # Nsight profiling\n",
    "    parser.add_argument(\"--profiling\", action='store_true')\n",
    "   \n",
    "    ##phase 1\n",
    "    parser.add_argument('--task_phase1', type=str, default='autoencoder_reconstruction')\n",
    "    parser.add_argument('--batch_size_phase1', type=int, default=8, help='for DDP, each GPU processes batch_size_pahse1 samples') #이걸.. 잘게 쪼개볼까? 원래는 4였음.\n",
    "    parser.add_argument('--validation_frequency_phase1', type=int, default=10000000) # 11 for test #original: 10000) #원래는 1000이었음 -> 약 7분 걸릴 예정.\n",
    "    parser.add_argument('--nEpochs_phase1', type=int, default=20) #epoch는 10개인 걸로~\n",
    "    parser.add_argument('--augment_prob_phase1', default=0)\n",
    "    parser.add_argument('--optim_phase1', default='AdamW')\n",
    "    parser.add_argument('--weight_decay_phase1', default=1e-7)\n",
    "    parser.add_argument('--lr_policy_phase1', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase1', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_gamma_phase1', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase1', type=int, default=500)\n",
    "    parser.add_argument('--lr_warmup_phase1', type=int, default=500)\n",
    "\n",
    "    ##phase 2\n",
    "    parser.add_argument('--task_phase2', type=str, default='transformer_reconstruction')\n",
    "    parser.add_argument('--batch_size_phase2', type=int, default=4) #원래는 1이었음\n",
    "    parser.add_argument('--validation_frequency_phase2', type=int, default=10000000) # 11 for test original: 10000) #원래는 500이었음\n",
    "    parser.add_argument('--optim_phase2', default='Adam')\n",
    "    parser.add_argument('--nEpochs_phase2', type=int, default=20)\n",
    "    parser.add_argument('--augment_prob_phase2', default=0)\n",
    "    parser.add_argument('--weight_decay_phase2', default=1e-7)\n",
    "    parser.add_argument('--lr_policy_phase2', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase2', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase2', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase2', type=int, default=1000)\n",
    "    parser.add_argument('--lr_warmup_phase2', type=int, default=500)\n",
    "    parser.add_argument('--model_weights_path_phase1', default=None)\n",
    "    parser.add_argument('--use_cont_loss', default=False)\n",
    "    parser.add_argument('--use_mask_loss', default=False)\n",
    "\n",
    "    ##phase 3\n",
    "    parser.add_argument('--task_phase3', type=str, default='fine_tune')\n",
    "    parser.add_argument('--batch_size_phase3', type=int, default=4) #원래는 3이었음\n",
    "    parser.add_argument('--validation_frequency_phase3', type=int, default=10000) # 11 for test # original: 10000) #원래는 200이었음\n",
    "    parser.add_argument('--nEpochs_phase3', type=int, default=20)\n",
    "    parser.add_argument('--augment_prob_phase3', default=0)\n",
    "    parser.add_argument('--optim_phase3', default='Adam')\n",
    "    parser.add_argument('--weight_decay_phase3', default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase3', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase3', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase3', type=float, default=0.9)\n",
    "    parser.add_argument('--lr_step_phase3', type=int, default=1500)\n",
    "    parser.add_argument('--lr_warmup_phase3', type=int, default=100)\n",
    "    parser.add_argument('--model_weights_path_phase2', default=None)\n",
    "    \n",
    "    ##phase 4 (test)\n",
    "    parser.add_argument('--task_phase4', type=str, default='test')\n",
    "    parser.add_argument('--model_weights_path_phase3', default=None)\n",
    "    parser.add_argument('--batch_size_phase4', type=int, default=4)\n",
    "    parser.add_argument('--nEpochs_phase4', type=int, default=20)\n",
    "    parser.add_argument('--augment_prob_phase4', default=0)\n",
    "    parser.add_argument('--optim_phase4', default='Adam')\n",
    "    parser.add_argument('--weight_decay_phase4', default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase4', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase4', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase4', type=float, default=0.9)\n",
    "    parser.add_argument('--lr_step_phase4', type=int, default=1500)\n",
    "    parser.add_argument('--lr_warmup_phase4', type=int, default=100)\n",
    "    \n",
    "    temp_args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Set dataset-specific Arguments\n",
    "    Dataset = fMRIDataModule\n",
    "    \n",
    "    parser = Dataset.add_data_specific_args(parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('2', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(Encoder_Transformer_Decoder(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(Encoder_Transformer_finetune(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(MobileNet_v2_Transformer_finetune(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "start=time.time()\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(MobileNet_v3_Transformer_finetune(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 4\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('2', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(Encoder_Transformer_Decoder(dim=(1,96,96,96),**kwargs), input_size=(8,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 4\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(Encoder_Transformer_finetune(dim=(1,96,96,96),**kwargs), input_size=(4,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(MobileNet_v2_Transformer_finetune(dim=(1,96,96,96),**kwargs), input_size=(4,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3',\n",
    "                 vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(MobileNet_v3_Transformer_finetune(dim=(1,96,96,96),**kwargs), input_size=(4,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
