{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lus/grand/projects/STlearn/4D_fMRI_Transformer/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lus/grand/projects/STlearn/4D_fMRI_Transformer\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_model_summary\n",
    "import torch\n",
    "from modules.model import Encoder_Transformer_Decoder,Encoder_Transformer_finetune,AutoEncoder, MobileNet_v2_Transformer_finetune,MobileNet_v3_Transformer_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from modules.data_preprocess_and_load.data_module3 import fMRIDataModule\n",
    "import os\n",
    "def get_arguments(base_path):\n",
    "    \"\"\"\n",
    "    handle arguments from commandline.\n",
    "    some other hyper parameters can only be changed manually (such as model architecture,dropout,etc)\n",
    "    notice some arguments are global and take effect for the entire three phase training process, while others are determined per phase\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser(add_help=False, formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--exp_name', type=str,default=\"baseline\") \n",
    "    parser.add_argument('--base_path', default=base_path)\n",
    "    parser.add_argument('--step', default='1', choices=['1','2','3','4'], help='which step you want to run')\n",
    "    \n",
    "    parser.add_argument('--cuda', default=True)\n",
    "    parser.add_argument('--log_dir', type=str, default=os.path.join(base_path, 'runs'))\n",
    "    \n",
    "    \n",
    "    # parser.add_argument('--random_TR', action='store_false') #True면(인자를 넣어주지 않으면) 전체 sequence 로부터 random sampling(default). False면 (--random_TR 인자를 넣어주면) 0번째 TR부터 sliding window\n",
    "    \n",
    "    # loss-related\n",
    "    parser.add_argument('--intensity_factor', default=1)\n",
    "    parser.add_argument('--perceptual_factor', default=1)\n",
    "    parser.add_argument('--which_perceptual', default='vgg', choices=['vgg','densenet3d'])\n",
    "    parser.add_argument('--reconstruction_factor', default=1)\n",
    "    \n",
    "    # model related\n",
    "    parser.add_argument('--transformer_hidden_layers', type=int,default=16)\n",
    "    parser.add_argument('--transformer_num_attention_heads',type=int, default=16)\n",
    "    parser.add_argument('--transformer_emb_size',type=int ,default=2640)\n",
    "    parser.add_argument('--running_mean_size', default=5000)\n",
    "    \n",
    "    # DDP configs:\n",
    "    parser.add_argument('--world_size', default=-1, type=int, \n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int, \n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int, \n",
    "                        help='local rank for distributed training')\n",
    "    parser.add_argument('--dist_backend', default='nccl', type=str, \n",
    "                        help='distributed backend')\n",
    "    parser.add_argument('--init_method', default='env', type=str, choices=['file','env'], help='DDP init method')\n",
    "    \n",
    "\n",
    "    # AMP configs:\n",
    "    parser.add_argument('--amp', action='store_false')\n",
    "    parser.add_argument('--gradient_clipping', action='store_true')\n",
    "    #parser.add_argument('--opt_level', default='O1', type=str,\n",
    "    #                    help='opt level of amp. O1 is recommended')\n",
    "    \n",
    "    # Gradient accumulation\n",
    "    parser.add_argument(\"--accumulation_steps\", default=1, type=int,required=False,help='mini batch size == accumulation_steps * args.train_batch_size')\n",
    "    \n",
    "    # Nsight profiling\n",
    "    parser.add_argument(\"--profiling\", action='store_true')\n",
    "   \n",
    "    ##phase 1\n",
    "    parser.add_argument('--task_phase1', type=str, default='autoencoder_reconstruction')\n",
    "    parser.add_argument('--batch_size_phase1', type=int, default=8, help='for DDP, each GPU processes batch_size_pahse1 samples') #이걸.. 잘게 쪼개볼까? 원래는 4였음.\n",
    "    parser.add_argument('--validation_frequency_phase1', type=int, default=10000000) # 11 for test #original: 10000) #원래는 1000이었음 -> 약 7분 걸릴 예정.\n",
    "    parser.add_argument('--nEpochs_phase1', type=int, default=20) #epoch는 10개인 걸로~\n",
    "    parser.add_argument('--augment_prob_phase1', default=0)\n",
    "    parser.add_argument('--optim_phase1', default='AdamW')\n",
    "    parser.add_argument('--weight_decay_phase1', default=1e-7)\n",
    "    parser.add_argument('--lr_policy_phase1', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase1', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_gamma_phase1', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase1', type=int, default=500)\n",
    "    parser.add_argument('--lr_warmup_phase1', type=int, default=500)\n",
    "\n",
    "    ##phase 2\n",
    "    parser.add_argument('--task_phase2', type=str, default='transformer_reconstruction')\n",
    "    parser.add_argument('--batch_size_phase2', type=int, default=4) #원래는 1이었음\n",
    "    parser.add_argument('--validation_frequency_phase2', type=int, default=10000000) # 11 for test original: 10000) #원래는 500이었음\n",
    "    parser.add_argument('--optim_phase2', default='Adam')\n",
    "    parser.add_argument('--nEpochs_phase2', type=int, default=20)\n",
    "    parser.add_argument('--augment_prob_phase2', default=0)\n",
    "    parser.add_argument('--weight_decay_phase2', default=1e-7)\n",
    "    parser.add_argument('--lr_policy_phase2', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase2', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase2', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase2', type=int, default=1000)\n",
    "    parser.add_argument('--lr_warmup_phase2', type=int, default=500)\n",
    "    parser.add_argument('--model_weights_path_phase1', default=None)\n",
    "    parser.add_argument('--use_cont_loss', default=False)\n",
    "    parser.add_argument('--use_mask_loss', default=False)\n",
    "\n",
    "    ##phase 3\n",
    "    parser.add_argument('--task_phase3', type=str, default='fine_tune')\n",
    "    parser.add_argument('--batch_size_phase3', type=int, default=4) #원래는 3이었음\n",
    "    parser.add_argument('--validation_frequency_phase3', type=int, default=10000) # 11 for test # original: 10000) #원래는 200이었음\n",
    "    parser.add_argument('--nEpochs_phase3', type=int, default=20)\n",
    "    parser.add_argument('--augment_prob_phase3', default=0)\n",
    "    parser.add_argument('--optim_phase3', default='Adam')\n",
    "    parser.add_argument('--weight_decay_phase3', default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase3', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase3', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase3', type=float, default=0.9)\n",
    "    parser.add_argument('--lr_step_phase3', type=int, default=1500)\n",
    "    parser.add_argument('--lr_warmup_phase3', type=int, default=100)\n",
    "    parser.add_argument('--model_weights_path_phase2', default=None)\n",
    "    \n",
    "    ##phase 4 (test)\n",
    "    parser.add_argument('--task_phase4', type=str, default='test')\n",
    "    parser.add_argument('--model_weights_path_phase3', default=None)\n",
    "    parser.add_argument('--batch_size_phase4', type=int, default=4)\n",
    "    parser.add_argument('--nEpochs_phase4', type=int, default=20)\n",
    "    parser.add_argument('--augment_prob_phase4', default=0)\n",
    "    parser.add_argument('--optim_phase4', default='Adam')\n",
    "    parser.add_argument('--weight_decay_phase4', default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase4', default='step', choices=['step','SGDR'], help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase4', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase4', type=float, default=0.9)\n",
    "    parser.add_argument('--lr_step_phase4', type=int, default=1500)\n",
    "    parser.add_argument('--lr_warmup_phase4', type=int, default=100)\n",
    "    \n",
    "    temp_args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Set dataset-specific Arguments\n",
    "    Dataset = fMRIDataModule\n",
    "    \n",
    "    parser = Dataset.add_data_specific_args(parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('2', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(Encoder_Transformer_Decoder(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "      Layer (type)              Input Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "          Conv3d-1      [80, 1, 96, 96, 96]             112             112\n",
      "       Dropout3d-2      [80, 4, 96, 96, 96]               0               0\n",
      "       GroupNorm-3      [80, 4, 96, 96, 96]               8               8\n",
      "       LeakyReLU-4      [80, 4, 96, 96, 96]               0               0\n",
      "          Conv3d-5      [80, 4, 96, 96, 96]             436             436\n",
      "       GroupNorm-6      [80, 4, 96, 96, 96]               8               8\n",
      "       LeakyReLU-7      [80, 4, 96, 96, 96]               0               0\n",
      "          Conv3d-8      [80, 4, 96, 96, 96]             436             436\n",
      "          Conv3d-9      [80, 4, 96, 96, 96]             872             872\n",
      "      GroupNorm-10      [80, 8, 48, 48, 48]              16              16\n",
      "      LeakyReLU-11      [80, 8, 48, 48, 48]               0               0\n",
      "         Conv3d-12      [80, 8, 48, 48, 48]           1,736           1,736\n",
      "      GroupNorm-13      [80, 8, 48, 48, 48]              16              16\n",
      "      LeakyReLU-14      [80, 8, 48, 48, 48]               0               0\n",
      "         Conv3d-15      [80, 8, 48, 48, 48]           1,736           1,736\n",
      "      GroupNorm-16      [80, 8, 48, 48, 48]              16              16\n",
      "      LeakyReLU-17      [80, 8, 48, 48, 48]               0               0\n",
      "         Conv3d-18      [80, 8, 48, 48, 48]           1,736           1,736\n",
      "      GroupNorm-19      [80, 8, 48, 48, 48]              16              16\n",
      "      LeakyReLU-20      [80, 8, 48, 48, 48]               0               0\n",
      "         Conv3d-21      [80, 8, 48, 48, 48]           1,736           1,736\n",
      "         Conv3d-22      [80, 8, 48, 48, 48]           3,472           3,472\n",
      "      GroupNorm-23     [80, 16, 24, 24, 24]              32              32\n",
      "      LeakyReLU-24     [80, 16, 24, 24, 24]               0               0\n",
      "         Conv3d-25     [80, 16, 24, 24, 24]           6,928           6,928\n",
      "      GroupNorm-26     [80, 16, 24, 24, 24]              32              32\n",
      "      LeakyReLU-27     [80, 16, 24, 24, 24]               0               0\n",
      "         Conv3d-28     [80, 16, 24, 24, 24]           6,928           6,928\n",
      "      GroupNorm-29     [80, 16, 24, 24, 24]              32              32\n",
      "      LeakyReLU-30     [80, 16, 24, 24, 24]               0               0\n",
      "         Conv3d-31     [80, 16, 24, 24, 24]           6,928           6,928\n",
      "      GroupNorm-32     [80, 16, 24, 24, 24]              32              32\n",
      "      LeakyReLU-33     [80, 16, 24, 24, 24]               0               0\n",
      "         Conv3d-34     [80, 16, 24, 24, 24]           6,928           6,928\n",
      "         Conv3d-35     [80, 16, 24, 24, 24]          13,856          13,856\n",
      "      GroupNorm-36     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-37     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-38     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-39     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-40     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-41     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-42     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-43     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-44     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-45     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-46     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-47     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-48     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-49     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-50     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-51     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-52     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-53     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-54     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-55     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-56     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-57     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-58     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-59     [80, 32, 12, 12, 12]          27,680          27,680\n",
      "      GroupNorm-60     [80, 32, 12, 12, 12]              64              64\n",
      "      LeakyReLU-61     [80, 32, 12, 12, 12]               0               0\n",
      "         Conv3d-62     [80, 32, 12, 12, 12]           1,730           1,730\n",
      "        Flatten-63      [80, 2, 12, 12, 12]               0               0\n",
      "         Linear-64               [80, 3456]       9,126,480       9,126,480\n",
      "         Linear-65             [4, 1, 2640]       6,972,240       6,972,240\n",
      "      LeakyReLU-66             [4, 1, 2640]               0               0\n",
      "      Embedding-67                  [4, 21]           5,280           5,280\n",
      "      Embedding-68                  [1, 21]          55,440          55,440\n",
      "      LayerNorm-69            [4, 21, 2640]           5,280           5,280\n",
      "        Dropout-70            [4, 21, 2640]               0               0\n",
      "         Linear-71            [4, 21, 2640]       6,972,240       6,972,240\n",
      "         Linear-72            [4, 21, 2640]       6,972,240       6,972,240\n",
      "         Linear-73            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Dropout-74          [4, 16, 21, 21]               0               0\n",
      "         Linear-75            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Dropout-76            [4, 21, 2640]               0               0\n",
      "      LayerNorm-77            [4, 21, 2640]           5,280           5,280\n",
      "         Linear-78            [4, 21, 2640]       8,113,152       8,113,152\n",
      "         Linear-79            [4, 21, 3072]       8,112,720       8,112,720\n",
      "        Dropout-80            [4, 21, 2640]               0               0\n",
      "      LayerNorm-81            [4, 21, 2640]           5,280           5,280\n",
      "         Linear-82            [4, 21, 2640]       6,972,240       6,972,240\n",
      "         Linear-83            [4, 21, 2640]       6,972,240       6,972,240\n",
      "         Linear-84            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Dropout-85          [4, 16, 21, 21]               0               0\n",
      "         Linear-86            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Dropout-87            [4, 21, 2640]               0               0\n",
      "      LayerNorm-88            [4, 21, 2640]           5,280           5,280\n",
      "         Linear-89            [4, 21, 2640]       8,113,152       8,113,152\n",
      "         Linear-90            [4, 21, 3072]       8,112,720       8,112,720\n",
      "        Dropout-91            [4, 21, 2640]               0               0\n",
      "      LayerNorm-92            [4, 21, 2640]           5,280           5,280\n",
      "         Linear-93            [4, 21, 2640]       6,972,240       6,972,240\n",
      "         Linear-94            [4, 21, 2640]       6,972,240       6,972,240\n",
      "         Linear-95            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Dropout-96          [4, 16, 21, 21]               0               0\n",
      "         Linear-97            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Dropout-98            [4, 21, 2640]               0               0\n",
      "      LayerNorm-99            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-100            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-101            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-102            [4, 21, 2640]               0               0\n",
      "     LayerNorm-103            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-104            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-105            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-106            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-107          [4, 16, 21, 21]               0               0\n",
      "        Linear-108            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-109            [4, 21, 2640]               0               0\n",
      "     LayerNorm-110            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-111            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-112            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-113            [4, 21, 2640]               0               0\n",
      "     LayerNorm-114            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-115            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-116            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-117            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-118          [4, 16, 21, 21]               0               0\n",
      "        Linear-119            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-120            [4, 21, 2640]               0               0\n",
      "     LayerNorm-121            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-122            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-123            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-124            [4, 21, 2640]               0               0\n",
      "     LayerNorm-125            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-126            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-127            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-128            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-129          [4, 16, 21, 21]               0               0\n",
      "        Linear-130            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-131            [4, 21, 2640]               0               0\n",
      "     LayerNorm-132            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-133            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-134            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-135            [4, 21, 2640]               0               0\n",
      "     LayerNorm-136            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-137            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-138            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-139            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-140          [4, 16, 21, 21]               0               0\n",
      "        Linear-141            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-142            [4, 21, 2640]               0               0\n",
      "     LayerNorm-143            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-144            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-145            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-146            [4, 21, 2640]               0               0\n",
      "     LayerNorm-147            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-148            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-149            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-150            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-151          [4, 16, 21, 21]               0               0\n",
      "        Linear-152            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-153            [4, 21, 2640]               0               0\n",
      "     LayerNorm-154            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-155            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-156            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-157            [4, 21, 2640]               0               0\n",
      "     LayerNorm-158            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-159            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-160            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-161            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-162          [4, 16, 21, 21]               0               0\n",
      "        Linear-163            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-164            [4, 21, 2640]               0               0\n",
      "     LayerNorm-165            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-166            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-167            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-168            [4, 21, 2640]               0               0\n",
      "     LayerNorm-169            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-170            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-171            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-172            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-173          [4, 16, 21, 21]               0               0\n",
      "        Linear-174            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-175            [4, 21, 2640]               0               0\n",
      "     LayerNorm-176            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-177            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-178            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-179            [4, 21, 2640]               0               0\n",
      "     LayerNorm-180            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-181            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-182            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-183            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-184          [4, 16, 21, 21]               0               0\n",
      "        Linear-185            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-186            [4, 21, 2640]               0               0\n",
      "     LayerNorm-187            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-188            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-189            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-190            [4, 21, 2640]               0               0\n",
      "     LayerNorm-191            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-192            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-193            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-194            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-195          [4, 16, 21, 21]               0               0\n",
      "        Linear-196            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-197            [4, 21, 2640]               0               0\n",
      "     LayerNorm-198            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-199            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-200            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-201            [4, 21, 2640]               0               0\n",
      "     LayerNorm-202            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-203            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-204            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-205            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-206          [4, 16, 21, 21]               0               0\n",
      "        Linear-207            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-208            [4, 21, 2640]               0               0\n",
      "     LayerNorm-209            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-210            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-211            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-212            [4, 21, 2640]               0               0\n",
      "     LayerNorm-213            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-214            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-215            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-216            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-217          [4, 16, 21, 21]               0               0\n",
      "        Linear-218            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-219            [4, 21, 2640]               0               0\n",
      "     LayerNorm-220            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-221            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-222            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-223            [4, 21, 2640]               0               0\n",
      "     LayerNorm-224            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-225            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-226            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-227            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-228          [4, 16, 21, 21]               0               0\n",
      "        Linear-229            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-230            [4, 21, 2640]               0               0\n",
      "     LayerNorm-231            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-232            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-233            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-234            [4, 21, 2640]               0               0\n",
      "     LayerNorm-235            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-236            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-237            [4, 21, 2640]       6,972,240       6,972,240\n",
      "        Linear-238            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-239          [4, 16, 21, 21]               0               0\n",
      "        Linear-240            [4, 21, 2640]       6,972,240       6,972,240\n",
      "       Dropout-241            [4, 21, 2640]               0               0\n",
      "     LayerNorm-242            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-243            [4, 21, 2640]       8,113,152       8,113,152\n",
      "        Linear-244            [4, 21, 3072]       8,112,720       8,112,720\n",
      "       Dropout-245            [4, 21, 2640]               0               0\n",
      "     LayerNorm-246            [4, 21, 2640]           5,280           5,280\n",
      "        Linear-247                [4, 2640]       6,972,240       6,972,240\n",
      "          Tanh-248                [4, 2640]               0               0\n",
      "        Linear-249                [4, 2640]           2,641           2,641\n",
      "============================================================================\n",
      "Total params: 729,423,667\n",
      "Trainable params: 729,423,667\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(Encoder_Transformer_finetune(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(MobileNet_v2_Transformer_finetune(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "           Layer (type)              Input Shape         Param #     Tr. Param #\n",
      "=================================================================================\n",
      "               Conv3d-1      [80, 1, 96, 96, 96]             432             432\n",
      "          BatchNorm3d-2     [80, 16, 48, 48, 48]              32              32\n",
      "               Hswish-3     [80, 16, 48, 48, 48]               0               0\n",
      "            Dropout3d-4     [80, 16, 48, 48, 48]               0               0\n",
      "               Conv3d-5     [80, 16, 48, 48, 48]             128             128\n",
      "          BatchNorm3d-6      [80, 8, 48, 48, 48]              16              16\n",
      "                 ReLU-7      [80, 8, 48, 48, 48]               0               0\n",
      "               Conv3d-8      [80, 8, 48, 48, 48]             216             216\n",
      "          BatchNorm3d-9      [80, 8, 24, 24, 24]              16              16\n",
      "   AdaptiveAvgPool3d-10      [80, 8, 24, 24, 24]               0               0\n",
      "              Linear-11                  [80, 8]              16              16\n",
      "                ReLU-12                  [80, 2]               0               0\n",
      "              Linear-13                  [80, 2]              16              16\n",
      "            Hsigmoid-14                  [80, 8]               0               0\n",
      "                ReLU-15      [80, 8, 24, 24, 24]               0               0\n",
      "              Conv3d-16      [80, 8, 24, 24, 24]              64              64\n",
      "         BatchNorm3d-17      [80, 8, 24, 24, 24]              16              16\n",
      "              Conv3d-18      [80, 8, 24, 24, 24]             320             320\n",
      "         BatchNorm3d-19     [80, 40, 24, 24, 24]              80              80\n",
      "                ReLU-20     [80, 40, 24, 24, 24]               0               0\n",
      "              Conv3d-21     [80, 40, 24, 24, 24]           1,080           1,080\n",
      "         BatchNorm3d-22     [80, 40, 12, 12, 12]              80              80\n",
      "            Identity-23     [80, 40, 12, 12, 12]               0               0\n",
      "                ReLU-24     [80, 40, 12, 12, 12]               0               0\n",
      "              Conv3d-25     [80, 40, 12, 12, 12]             640             640\n",
      "         BatchNorm3d-26     [80, 16, 12, 12, 12]              32              32\n",
      "              Conv3d-27     [80, 16, 12, 12, 12]             768             768\n",
      "         BatchNorm3d-28     [80, 48, 12, 12, 12]              96              96\n",
      "                ReLU-29     [80, 48, 12, 12, 12]               0               0\n",
      "              Conv3d-30     [80, 48, 12, 12, 12]           1,296           1,296\n",
      "         BatchNorm3d-31     [80, 48, 12, 12, 12]              96              96\n",
      "            Identity-32     [80, 48, 12, 12, 12]               0               0\n",
      "                ReLU-33     [80, 48, 12, 12, 12]               0               0\n",
      "              Conv3d-34     [80, 48, 12, 12, 12]             768             768\n",
      "         BatchNorm3d-35     [80, 16, 12, 12, 12]              32              32\n",
      "              Conv3d-36     [80, 16, 12, 12, 12]             768             768\n",
      "         BatchNorm3d-37     [80, 48, 12, 12, 12]              96              96\n",
      "              Hswish-38     [80, 48, 12, 12, 12]               0               0\n",
      "              Conv3d-39     [80, 48, 12, 12, 12]           6,000           6,000\n",
      "         BatchNorm3d-40        [80, 48, 6, 6, 6]              96              96\n",
      "   AdaptiveAvgPool3d-41        [80, 48, 6, 6, 6]               0               0\n",
      "              Linear-42                 [80, 48]             576             576\n",
      "                ReLU-43                 [80, 12]               0               0\n",
      "              Linear-44                 [80, 12]             576             576\n",
      "            Hsigmoid-45                 [80, 48]               0               0\n",
      "              Hswish-46        [80, 48, 6, 6, 6]               0               0\n",
      "              Conv3d-47        [80, 48, 6, 6, 6]           1,152           1,152\n",
      "         BatchNorm3d-48        [80, 24, 6, 6, 6]              48              48\n",
      "              Conv3d-49        [80, 24, 6, 6, 6]           2,304           2,304\n",
      "         BatchNorm3d-50        [80, 96, 6, 6, 6]             192             192\n",
      "              Hswish-51        [80, 96, 6, 6, 6]               0               0\n",
      "   AdaptiveAvgPool3d-52        [80, 96, 6, 6, 6]               0               0\n",
      "              Conv3d-53        [80, 96, 1, 1, 1]         256,080         256,080\n",
      "              Hswish-54      [80, 2640, 1, 1, 1]               0               0\n",
      "              Linear-55             [4, 1, 2640]       6,972,240       6,972,240\n",
      "           LeakyReLU-56             [4, 1, 2640]               0               0\n",
      "           Embedding-57                  [4, 21]           5,280           5,280\n",
      "           Embedding-58                  [1, 21]          55,440          55,440\n",
      "           LayerNorm-59            [4, 21, 2640]           5,280           5,280\n",
      "             Dropout-60            [4, 21, 2640]               0               0\n",
      "              Linear-61            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-62            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-63            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-64          [4, 16, 21, 21]               0               0\n",
      "              Linear-65            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-66            [4, 21, 2640]               0               0\n",
      "           LayerNorm-67            [4, 21, 2640]           5,280           5,280\n",
      "              Linear-68            [4, 21, 2640]       8,113,152       8,113,152\n",
      "              Linear-69            [4, 21, 3072]       8,112,720       8,112,720\n",
      "             Dropout-70            [4, 21, 2640]               0               0\n",
      "           LayerNorm-71            [4, 21, 2640]           5,280           5,280\n",
      "              Linear-72            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-73            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-74            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-75          [4, 16, 21, 21]               0               0\n",
      "              Linear-76            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-77            [4, 21, 2640]               0               0\n",
      "           LayerNorm-78            [4, 21, 2640]           5,280           5,280\n",
      "              Linear-79            [4, 21, 2640]       8,113,152       8,113,152\n",
      "              Linear-80            [4, 21, 3072]       8,112,720       8,112,720\n",
      "             Dropout-81            [4, 21, 2640]               0               0\n",
      "           LayerNorm-82            [4, 21, 2640]           5,280           5,280\n",
      "              Linear-83            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-84            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-85            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-86          [4, 16, 21, 21]               0               0\n",
      "              Linear-87            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-88            [4, 21, 2640]               0               0\n",
      "           LayerNorm-89            [4, 21, 2640]           5,280           5,280\n",
      "              Linear-90            [4, 21, 2640]       8,113,152       8,113,152\n",
      "              Linear-91            [4, 21, 3072]       8,112,720       8,112,720\n",
      "             Dropout-92            [4, 21, 2640]               0               0\n",
      "           LayerNorm-93            [4, 21, 2640]           5,280           5,280\n",
      "              Linear-94            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-95            [4, 21, 2640]       6,972,240       6,972,240\n",
      "              Linear-96            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-97          [4, 16, 21, 21]               0               0\n",
      "              Linear-98            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Dropout-99            [4, 21, 2640]               0               0\n",
      "          LayerNorm-100            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-101            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-102            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-103            [4, 21, 2640]               0               0\n",
      "          LayerNorm-104            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-105            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-106            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-107            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-108          [4, 16, 21, 21]               0               0\n",
      "             Linear-109            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-110            [4, 21, 2640]               0               0\n",
      "          LayerNorm-111            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-112            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-113            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-114            [4, 21, 2640]               0               0\n",
      "          LayerNorm-115            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-116            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-117            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-118            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-119          [4, 16, 21, 21]               0               0\n",
      "             Linear-120            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-121            [4, 21, 2640]               0               0\n",
      "          LayerNorm-122            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-123            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-124            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-125            [4, 21, 2640]               0               0\n",
      "          LayerNorm-126            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-127            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-128            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-129            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-130          [4, 16, 21, 21]               0               0\n",
      "             Linear-131            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-132            [4, 21, 2640]               0               0\n",
      "          LayerNorm-133            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-134            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-135            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-136            [4, 21, 2640]               0               0\n",
      "          LayerNorm-137            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-138            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-139            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-140            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-141          [4, 16, 21, 21]               0               0\n",
      "             Linear-142            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-143            [4, 21, 2640]               0               0\n",
      "          LayerNorm-144            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-145            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-146            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-147            [4, 21, 2640]               0               0\n",
      "          LayerNorm-148            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-149            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-150            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-151            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-152          [4, 16, 21, 21]               0               0\n",
      "             Linear-153            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-154            [4, 21, 2640]               0               0\n",
      "          LayerNorm-155            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-156            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-157            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-158            [4, 21, 2640]               0               0\n",
      "          LayerNorm-159            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-160            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-161            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-162            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-163          [4, 16, 21, 21]               0               0\n",
      "             Linear-164            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-165            [4, 21, 2640]               0               0\n",
      "          LayerNorm-166            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-167            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-168            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-169            [4, 21, 2640]               0               0\n",
      "          LayerNorm-170            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-171            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-172            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-173            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-174          [4, 16, 21, 21]               0               0\n",
      "             Linear-175            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-176            [4, 21, 2640]               0               0\n",
      "          LayerNorm-177            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-178            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-179            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-180            [4, 21, 2640]               0               0\n",
      "          LayerNorm-181            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-182            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-183            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-184            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-185          [4, 16, 21, 21]               0               0\n",
      "             Linear-186            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-187            [4, 21, 2640]               0               0\n",
      "          LayerNorm-188            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-189            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-190            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-191            [4, 21, 2640]               0               0\n",
      "          LayerNorm-192            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-193            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-194            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-195            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-196          [4, 16, 21, 21]               0               0\n",
      "             Linear-197            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-198            [4, 21, 2640]               0               0\n",
      "          LayerNorm-199            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-200            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-201            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-202            [4, 21, 2640]               0               0\n",
      "          LayerNorm-203            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-204            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-205            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-206            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-207          [4, 16, 21, 21]               0               0\n",
      "             Linear-208            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-209            [4, 21, 2640]               0               0\n",
      "          LayerNorm-210            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-211            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-212            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-213            [4, 21, 2640]               0               0\n",
      "          LayerNorm-214            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-215            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-216            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-217            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-218          [4, 16, 21, 21]               0               0\n",
      "             Linear-219            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-220            [4, 21, 2640]               0               0\n",
      "          LayerNorm-221            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-222            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-223            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-224            [4, 21, 2640]               0               0\n",
      "          LayerNorm-225            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-226            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-227            [4, 21, 2640]       6,972,240       6,972,240\n",
      "             Linear-228            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-229          [4, 16, 21, 21]               0               0\n",
      "             Linear-230            [4, 21, 2640]       6,972,240       6,972,240\n",
      "            Dropout-231            [4, 21, 2640]               0               0\n",
      "          LayerNorm-232            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-233            [4, 21, 2640]       8,113,152       8,113,152\n",
      "             Linear-234            [4, 21, 3072]       8,112,720       8,112,720\n",
      "            Dropout-235            [4, 21, 2640]               0               0\n",
      "          LayerNorm-236            [4, 21, 2640]           5,280           5,280\n",
      "             Linear-237                [4, 2640]       6,972,240       6,972,240\n",
      "               Tanh-238                [4, 2640]               0               0\n",
      "             Linear-239                [4, 2640]           2,641           2,641\n",
      "=================================================================================\n",
      "Total params: 720,293,521\n",
      "Trainable params: 720,293,521\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "start=time.time()\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "print(pytorch_model_summary.summary(MobileNet_v3_Transformer_finetune(dim=(1,96,96,96),**kwargs), torch.zeros(4,1,96,96,96,20), max_depth=None, show_input=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 4\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('2', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(Encoder_Transformer_Decoder(dim=(1,96,96,96),**kwargs), input_size=(8,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "batch_size = 4\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(Encoder_Transformer_finetune(dim=(1,96,96,96),**kwargs), input_size=(4,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3', vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(MobileNet_v2_Transformer_finetune(dim=(1,96,96,96),**kwargs), input_size=(4,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from modules.utils import *\n",
    "import time\n",
    "from modules.utils import *\n",
    "\n",
    "base_path = os.getcwd() \n",
    "\n",
    "args = get_arguments(base_path)\n",
    "args = sort_args('3',\n",
    "                 vars(args))\n",
    "\n",
    "kwargs = args\n",
    "summary(MobileNet_v3_Transformer_finetune(dim=(1,96,96,96),**kwargs), input_size=(4,1,96,96,96,20),depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
